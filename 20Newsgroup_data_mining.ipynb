{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20Newsgroup-data-mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gandalf1819/20Newsgroups-data-mining/blob/master/20Newsgroup_data_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZCE_gBIbrCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://mirrors.viethosting.com/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56oZjnhobs4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7tk-1RwcFWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_yvDyphcHAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pyspark\n",
        "import string\n",
        "import csv\n",
        "import json\n",
        "import statistics\n",
        "from itertools import combinations\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as D\n",
        "from pyspark.sql.window import Window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yokZIv8cH9nq",
        "colab_type": "code",
        "outputId": "2e0d4bfa-fa9f-4654-8a10-9596fc8ef563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.6/dist-packages (0.1.1.36)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.17.4)\n",
            "Requirement already satisfied: scikit-image>=0.12; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from lime) (0.15.0)\n",
            "Requirement already satisfied: matplotlib; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from lime) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.3.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (4.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (2.4)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12; python_version >= \"3.0\"->lime) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib; python_version >= \"3.0\"->lime) (2.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (0.14.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit-image>=0.12; python_version >= \"3.0\"->lime) (0.46)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12; python_version >= \"3.0\"->lime) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib; python_version >= \"3.0\"->lime) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib; python_version >= \"3.0\"->lime) (41.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX0U_HWNJJ_r",
        "colab_type": "code",
        "outputId": "9fb32c99-7f80-44e1-9e2d-211c3af7cf8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEiZamEgJL8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "from pyspark.sql import types as D\n",
        "from pyspark.ml.feature import HashingTF, IDF, RegexTokenizer, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "import csv\n",
        "import lime \n",
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Qg1CXsJZoM",
        "colab_type": "code",
        "outputId": "b816f34b-06f2-40c5-bcd3-8798b60c35e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "sc = SparkContext()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-2dfc28fca47d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/content/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-10-2dfc28fca47d>:1 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYaFLlN1Jf72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"hw3\") \\\n",
        "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ccvQtOMJ6v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "categories = [\"alt.atheism\", \"soc.religion.christian\"]\n",
        "LabeledDocument = pyspark.sql.Row(\"category\", \"text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fN8ygYoJ9h_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categoryFromPath(path):\n",
        "    return path.split(\"/\")[-2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62tHhRHAJ99u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareDF(typ):\n",
        "    rdds = [sc.wholeTextFiles(\"/content/gdrive/My Drive/BigData-hw3/20news-bydate/20news-bydate-\" + typ + \"/\" + category)\\\n",
        "              .map(lambda x: LabeledDocument(categoryFromPath(x[0]), x[1]))\\\n",
        "            for category in categories]\n",
        "    return sc.union(rdds).toDF()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmhrPy7OJ_t2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = prepareDF(\"train\").cache()\n",
        "test_df  = prepareDF(\"test\").cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_BaH967KGVl",
        "colab_type": "text"
      },
      "source": [
        "Task 1.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gBsH1YbKCUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Append text to file\n",
        "def writeToFile(text):\n",
        "\twith open('/content/gdrive/My Drive/BigData-hw3/cnw282_report.txt', 'a', newline=\"\") as f:\n",
        "\t\tf.write(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaJKyuImKJM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df_rows = 'Number of rows in train_df = ' + str(train_df.count())+'\\n'\n",
        "test_df_rows = 'Number of rows in test_df = ' + str(test_df.count())+'\\n'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adQ9zpG7KLiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dump the results to the report\n",
        "writeToFile(\"Task 1.1 (a)\\n\")\n",
        "writeToFile(train_df_rows)\n",
        "writeToFile(test_df_rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjOI-oI8KM-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train_df.rdd.zipWithIndex()\n",
        "train_df = train_df.toDF()\n",
        "train_df = train_df.withColumn('category', train_df['_1'].getItem(\"category\"))\n",
        "train_df = train_df.withColumn('text', train_df['_1'].getItem(\"text\"))\n",
        "train_df = train_df.withColumnRenamed('_2', 'id')\n",
        "train_df = train_df.select('id','category','text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22B7dlYLKOqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = test_df.rdd.zipWithIndex()\n",
        "test_df = test_df.toDF()\n",
        "test_df = test_df.withColumn('category', test_df['_1'].getItem(\"category\"))\n",
        "test_df = test_df.withColumn('text', test_df['_1'].getItem(\"text\"))\n",
        "test_df = test_df.withColumnRenamed('_2', 'id')\n",
        "test_df = test_df.select('id','category','text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik2wCxqkKRc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writeToFile(\"\\nTask 1.1 (b)\\n\")\n",
        "writeToFile(\"First 5 rows of 'INDEXED' test set \\n\\n\")\n",
        "k = test_df.take(5)\n",
        "for i,row in enumerate(k):\n",
        "\trow_name='Row-' + str(i)\n",
        "\twriteToFile(row_name+'\\n')\n",
        "\twriteToFile(str(row[0])+', '+str(row[1])+ ', '+str(row[2])+ '\\n\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myC2qinQKTVT",
        "colab_type": "code",
        "outputId": "ef036976-5e8a-4edf-9f44-db70cc942822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Build pipeline and run\n",
        "indexer   = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
        "tokenizer = RegexTokenizer(pattern=u'\\W+', inputCol=\"text\", outputCol=\"words\", toLowercase=False)\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
        "idf       = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr        = LogisticRegression(maxIter=20, regParam=0.001)\n",
        "\n",
        "# Builing model pipeline\n",
        "pipeline = Pipeline(stages=[indexer, tokenizer, hashingTF, idf, lr])\n",
        "\n",
        "# Train model on training set\n",
        "model = pipeline.fit(train_df)   #if you give new names to your indexed datasets, make sure to make adjustments here\n",
        "\n",
        "# Model prediction on test set\n",
        "pred = model.transform(test_df)  # ...and here\n",
        "\n",
        "# Model prediction accuracy (F1-score)\n",
        "pl = pred.select(\"label\", \"prediction\").rdd.cache()\n",
        "metrics = MulticlassMetrics(pl)\n",
        "init_f1_score = metrics.fMeasure()\n",
        "print('f1 score: ',init_f1_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score:  0.9483960948396095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snex_XnSKTxi",
        "colab_type": "code",
        "outputId": "4efd96ef-19c9-4ed5-ef05-008d4195c720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        }
      },
      "source": [
        "# Dump f1 score to report\n",
        "writeToFile(\"\\nTask 1.2 (a)\\n\")\n",
        "writeToFile(\"F1 score: \")\n",
        "writeToFile(str(init_f1_score))\n",
        "\n",
        "pred.show()\n",
        "\n",
        "# Dump schema to report\n",
        "writeToFile(\"\\nTask 1.2 (b)\\n\")\n",
        "for tup in pred.dtypes:\n",
        "\tres='Column Name: ' + str(tup[0])+', '+ 'Type: ' + str(tup[1]) + '\\n'\n",
        "\twriteToFile(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "| id|   category|                text|label|               words|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
            "+---+-----------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|  0|alt.atheism|From: sandvik@new...|  1.0|[From, sandvik, n...|(262144,[1838,365...|(262144,[1838,365...|[-9.6765787643934...|[6.27318353598026...|       1.0|\n",
            "|  1|alt.atheism|From: kempmp@phoe...|  1.0|[From, kempmp, ph...|(262144,[14,619,1...|(262144,[14,619,1...|[1.91251820260104...|[0.87130179057815...|       0.0|\n",
            "|  2|alt.atheism|From: rsrodger@wa...|  1.0|[From, rsrodger, ...|(262144,[3030,330...|(262144,[3030,330...|[-1.5061726689323...|[0.18150669621399...|       1.0|\n",
            "|  3|alt.atheism|From: I3150101@db...|  1.0|[From, I3150101, ...|(262144,[1838,582...|(262144,[1838,582...|[-6.5512560164246...|[0.00142628328758...|       1.0|\n",
            "|  4|alt.atheism|From: bcash@crchh...|  1.0|[From, bcash, crc...|(262144,[5455,963...|(262144,[5455,963...|[-13.091925810941...|[2.06180679886528...|       1.0|\n",
            "|  5|alt.atheism|From: livesey@sol...|  1.0|[From, livesey, s...|(262144,[558,2522...|(262144,[558,2522...|[-8.9116406383763...|[1.34792305797664...|       1.0|\n",
            "|  6|alt.atheism|From: Alan.Olsen@...|  1.0|[From, Alan, Olse...|(262144,[1311,183...|(262144,[1311,183...|[-3.0544011804496...|[0.04502784183319...|       1.0|\n",
            "|  7|alt.atheism|From: livesey@sol...|  1.0|[From, livesey, s...|(262144,[230,353,...|(262144,[230,353,...|[-8.6043615818473...|[1.83270955395512...|       1.0|\n",
            "|  8|alt.atheism|From: sandvik@new...|  1.0|[From, sandvik, n...|(262144,[571,3924...|(262144,[571,3924...|[-6.0735024812579...|[0.00229780048210...|       1.0|\n",
            "|  9|alt.atheism|From: arromdee@jy...|  1.0|[From, arromdee, ...|(262144,[14,8630,...|(262144,[14,8630,...|[-0.7310678144283...|[0.32496044664450...|       1.0|\n",
            "| 10|alt.atheism|Subject: Re: Deat...|  1.0|[Subject, Re, Dea...|(262144,[314,1311...|(262144,[314,1311...|[-2.9201044790767...|[0.05116862817828...|       1.0|\n",
            "| 11|alt.atheism|From: aiken@unity...|  1.0|[From, aiken, uni...|(262144,[221,250,...|(262144,[221,250,...|[0.30427050277345...|[0.57548614334750...|       0.0|\n",
            "| 12|alt.atheism|From: livesey@sol...|  1.0|[From, livesey, s...|(262144,[14,558,2...|(262144,[14,558,2...|[-6.7584553346948...|[0.00115967477168...|       1.0|\n",
            "| 13|alt.atheism|From: sandvik@new...|  1.0|[From, sandvik, n...|(262144,[1311,905...|(262144,[1311,905...|[-4.8736912301642...|[0.00758708933555...|       1.0|\n",
            "| 14|alt.atheism|From: Alan.Olsen@...|  1.0|[From, Alan, Olse...|(262144,[1311,183...|(262144,[1311,183...|[0.62509225561139...|[0.65137581485758...|       0.0|\n",
            "| 15|alt.atheism|From: bobbe@vice....|  1.0|[From, bobbe, vic...|(262144,[9129,211...|(262144,[9129,211...|[-6.3028264301561...|[0.00182777503884...|       1.0|\n",
            "| 16|alt.atheism|From: livesey@sol...|  1.0|[From, livesey, s...|(262144,[558,5377...|(262144,[558,5377...|[-3.8762456217388...|[0.02030755668590...|       1.0|\n",
            "| 17|alt.atheism|From: I3150101@db...|  1.0|[From, I3150101, ...|(262144,[1311,252...|(262144,[1311,252...|[-7.7570282453547...|[4.27542918832804...|       1.0|\n",
            "| 18|alt.atheism|Subject: Re: Who ...|  1.0|[Subject, Re, Who...|(262144,[14,619,9...|(262144,[14,619,9...|[-2.6478926010470...|[0.06611901674891...|       1.0|\n",
            "| 19|alt.atheism|From: madhaus@net...|  1.0|[From, madhaus, n...|(262144,[14,2784,...|(262144,[14,2784,...|[-9.9796392424625...|[4.63316347071458...|       1.0|\n",
            "+---+-----------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4J2bjt5KcyK",
        "colab_type": "text"
      },
      "source": [
        "Task 1.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dNUsXlZKbku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use LIME to explain example\n",
        "class_names = ['Atheism', 'Christian']\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "# Choose a random text in test set, change seed for randomness \n",
        "test_point = test_df.sample(False, 0.1, seed = 10).limit(1)\n",
        "test_point_label = test_point.select(\"category\").collect()[0][0]\n",
        "test_point_text = test_point.select(\"text\").collect()[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-xSmzh6Kfpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classifier_fn(data):\n",
        "    spark_object = spark.createDataFrame(data, \"string\").toDF(\"text\")\n",
        "    pred = model.transform(spark_object)   #if you build the model with a different name, make appropriate changes here\n",
        "    output = np.array((pred.select(\"probability\").collect())).reshape(len(data),2)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m_gPXzMKi2p",
        "colab_type": "code",
        "outputId": "ab0898ee-a0ca-4129-fd86-c05b8d45c216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "exp = explainer.explain_instance(test_point_text, classifier_fn, num_features=6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujE8jggcKlC0",
        "colab_type": "text"
      },
      "source": [
        "Task 1.3<br>\n",
        "Task 1.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpyiOECWKjST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_IDS = pred.filter((F.col('id')==0)|(F.col('id')==275)|(F.col('id')==664)).collect()\n",
        "\n",
        "misclassified = pred.filter((F.col('label')!=F.col('prediction'))).collect()\n",
        "\n",
        "misclassified_list=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c1oNvbQKtCv",
        "colab_type": "code",
        "outputId": "7f5b7750-96aa-46fc-cf73-a76aa6046104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for row in misclassified:\n",
        "\tId = row[0]\n",
        "\tprob_l = row[8]\n",
        "\tconf = abs(prob_l[0]-prob_l[1])\n",
        "\texp_row = explainer.explain_instance(row[2], classifier_fn, num_features=6)\n",
        "\tnew_row = [Id, conf, exp_row.as_list()]\n",
        "\tmisclassified_list.append(new_row)\n",
        "\n",
        "misclassified_list.sort(key= lambda k: (k[1], k[0]), reverse=True)\n",
        "\n",
        "header=['ID', 'Confidence', 'Explanation-List']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVs9aAiUKxtj",
        "colab_type": "code",
        "outputId": "f85d0fd3-31c7-421b-819c-a3dc0bda79b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Task 1.4\n",
        "with open('/content/gdrive/My Drive/BigData-hw3/cnw282_misclassified_ordered.csv', \"w\", newline=\"\") as f:\n",
        "\twriter = csv.writer(f)\n",
        "\twriter.writerow(i for i in header)\n",
        "\twriter.writerows(misclassified_list)\n",
        "\n",
        "task_ID=[]\n",
        "for row in test_IDS:\n",
        "\tId=row[0]\n",
        "\tcategory=row[1]\n",
        "\tprob_list=row[8]\n",
        "\tpred_category='atheism' if row[9]==1.0 else 'christian'\n",
        "\texp_row=explainer.explain_instance(row[2], classifier_fn, num_features=6)\n",
        "\ttask_ID.append([Id, category, pred_category, prob_list, exp_row.as_list()])\n",
        "\n",
        "writeToFile(\"\\nTask 1.3\\n\")\n",
        "for row in task_ID:\n",
        "\tId='Id: '+ str(row[0])\n",
        "\tlabel='Actual Category: ' + str(row[1])\n",
        "\tpred_label='Predicted Category: ' + str(row[2])\n",
        "\tprob='Probability: ' + str(row[3])\n",
        "\texplanation='Explanation List: ' + str(row[4])\n",
        "\tline=Id +', '+label+', '+ pred_label+ ', '+prob+ ', '+ explanation+ '\\n'\n",
        "\twriteToFile(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5_pZrL7K2AS",
        "colab_type": "text"
      },
      "source": [
        "Task 1.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG6VdMn3K0lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = {}\n",
        "\n",
        "for row in misclassified_list:\n",
        "\texp_list = row[2]\n",
        "\tfor tup in exp_list:\n",
        "\t\tword = tup[0]\n",
        "\t\tweight = tup[1]\n",
        "\t\tif word not in words.keys():\n",
        "\t\t\twords[word]=[]\n",
        "\t\t\twords[word].append(1)\n",
        "\t\t\twords[word].append(abs(weight))\n",
        "\t\telif word in words.keys():\n",
        "\t\t\twords[word][0] = words[word][0]+1\n",
        "\t\t\twords[word][1] = words[word][1]+abs(weight)\n",
        "\n",
        "words_arr=[]\n",
        "\n",
        "words_new_arr=[]\n",
        "\n",
        "# For report\n",
        "for k, v in words.items():\n",
        "\tnew_row = [k, v[0], v[1]]\n",
        "\twords_arr.append(new_row)\n",
        "\n",
        "# For Task 2\n",
        "for k, v in words.items():\n",
        "\tn_row = [k, v[1]/v[0]]\n",
        "\twords_new_arr.append(n_row)\n",
        "\n",
        "words_new_arr.sort(key=lambda k: (k[1]), reverse=True)\t\n",
        "\n",
        "words_arr.sort(key=lambda k: (k[1], k[2]), reverse=True)\n",
        "header1 = ['Word', 'Count', 'Weight']\n",
        "\t\n",
        "# Task 1.5\n",
        "with open('/content/gdrive/My Drive/BigData-hw3/cnw282_words_weight.csv', 'w', newline=\"\") as f1:\n",
        "\twriter = csv.writer(f1)\n",
        "\twriter.writerow(i for i in header1)\n",
        "\twriter.writerows(words_arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRLxyCMcK7w9",
        "colab_type": "text"
      },
      "source": [
        "Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjcfQOBPK8QT",
        "colab_type": "code",
        "outputId": "4dac6368-263f-492d-af64-c6e737fbf6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "t5 = [row[0] for row in words_new_arr[:20]]\t\t\t\t\t\n",
        "remove_word = F.udf(lambda x: x.replace(t5[0], \"\").replace(t5[1], \"\").replace(t5[2], \"\").replace(t5[3], \"\").replace(t5[4],\"\").replace(t5[5],\"\").replace(t5[6],\"\").replace(t5[7],\"\").replace(t5[8],\"\").replace(t5[9],\"\").replace(t5[10], \"\").replace(t5[11], \"\").replace(t5[12],\"\").replace(t5[13],\"\")  , D.StringType())\n",
        "train_df = train_df.withColumn('text1', remove_word(train_df.text))\n",
        "train_df = train_df.select('id', 'category', 'text1')\n",
        "train_df = train_df.withColumnRenamed(\"text1\", \"text\")\n",
        "train_df.show(2)\n",
        "model = pipeline.fit(train_df)\n",
        "pred = model.transform(test_df)\n",
        "pl = pred.select(\"label\", \"prediction\").rdd.cache()\n",
        "metrics = MulticlassMetrics(pl)\n",
        "print('new f1 score = ', metrics.fMeasure())\n",
        "new_misclassified = pred.filter((F.col('label')!=F.col('prediction'))).collect()\n",
        "new_misclassified_IDs = [row[0] for row in new_misclassified]\n",
        "prev_missclassified_IDs = [row[0] for row in misclassified_list]\n",
        "correct_ids_after=[]\n",
        "\n",
        "for Id in prev_missclassified_IDs:\n",
        "\tif Id not in new_misclassified_IDs:\n",
        "\t\tcorrect_ids_after.append(Id)\n",
        "\n",
        "print(\"Final: \", correct_ids_after)\n",
        "\n",
        "writeToFile(\"\\n\\nTask 2\\n\")\n",
        "writeToFile(\"\\nStrategy:\\n\")\n",
        "writeToFile(\"Step-1:\\n\") \n",
        "writeToFile(\"We have identified the words which contributed towards misclassified documents. Using weights/count as measure, we will remove the misclassified words which contributed to a decrease in accuracy of the model. Along with this, we have the words in a sorted order in descending order starting with words whcih misclassified the most number of documents.\\n\") \n",
        "writeToFile(\"We can put the top 13 words in a list that had contributed the most towards misclassified document\\n\")\n",
        "writeToFile(\"\\nStep-2:\\n\") \n",
        "writeToFile(\"Create a new column called 'text1' from text with the use of user defined function - udf. Rename this column to 'text' later\\n\")\n",
        "writeToFile(\"\\nStep-3:\\n\")\n",
        "writeToFile(\"After making these modifications, we can observe that we get a better accuracy as we have optimized the model\\n\\n\")\n",
        "\n",
        "writeToFile(\"We can say that words that contributed to multiple misclassified documents contributed for the decrease of the precision of the model - Without loss of generality (WLOG)\\n\")\n",
        "writeToFile(\"After removing some of these words we reduce our rate of false positive (FPR) and false negatives (FNR) which will contribute to the increase in the F1 score\\n\\n\")\n",
        "\n",
        "acc = \"New Accuracy after Feature Engineering: \" + str(metrics.fMeasure())+'\\n'\n",
        "# Dump new accuracy to report\n",
        "writeToFile(acc)\n",
        "\n",
        "# Dump ID's that got classified correctly after feature selection\n",
        "correct_ids = \"Document ID's that are classified correctly after feature selection (which were misclassified before): \" + str(correct_ids_after)+'\\n'\n",
        "writeToFile(correct_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----------+--------------------+\n",
            "| id|   category|                text|\n",
            "+---+-----------+--------------------+\n",
            "|  0|alt.atheism|From: kv07@IASTAT...|\n",
            "|  1|alt.atheism|From: mas@Cadence...|\n",
            "+---+-----------+--------------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "new f1 score =  0.9595536959553695\n",
            "Final:  [165, 121, 288, 81, 160, 271, 259, 258, 275]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YsqF3poVl3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}